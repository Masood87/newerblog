---
title: Selection of Provinces for Monitoring in SAP 2018
author: Masood Sadat
date: '2018-06-10'
slug: selection-of-provinces-for-monitoring-in-sap-2018
categories:
  - afghanistan
tags:
  - AfghanSurvey
  - EDA
---

```{r include=FALSE}
library(tidyverse)
```

# Introduction

The Asia Foundation (TAF) employs a number of methods to ensure data quality for the Survey of the Afghan People (SAP). The most important quality controls are (1) third party monitoringy, and (2) logic tests. The third party monitoring is carried out on 10% of all surveys by an independent company, and 25% are subject to monitoring from the data collection company. The **third party monitoring** started in 2014, and since then Sayara Research has been the implementing partner for this activity. The **logic tests** are a set of internal tools in the form of codes that are used to detects inconsistencies and potentially fraudulent data. In this note, I document the analysis and reasoning behind the selection of 13 provinces for the _third party monitoring_, in which logic tests and previous year's third party monitoring report are used.

# Data

TAF aims to use monitoring to improve the quality of fieldwork and data, and employs monitoring in the most problematic provinces. In 2017, 13 provinces were monitored, and TAF wants to keep the same number of provinces for 2018 as well. To select 13 most problematic provinces, a number of documents and resources are consulted to finalize the list of provinces. The following are a list of documents and resources consulted for this task:

1. Logic test results in 2017
2. SAP sample in 2017
3. Third Party Monitoring report in 2017
4. Advices from the fieldwork and monitoring companies (ACSOR and Sayara)

# Analysis

```{r echo=FALSE, message=FALSE, warning=FALSE}
lt <- read_csv("~/Documents/SAP 2018/Contracts/Sayara/Province selection based on SAP 2017/SAP 2017 LT report by province.csv", skip = 2)
tpm <- read_csv("~/Documents/SAP 2018/Contracts/Sayara/Province selection based on SAP 2017/sayara scores.csv")

lt <- select(lt, prov., failures, n, sayara, `proportion of LT failures`) %>%
  mutate(`proportion of LT failures` = round(as.numeric(`proportion of LT failures`)*100, 2),
         sayara = factor(case_when(sayara == "0" ~ "Not Monitored", sayara == "1" ~ "Monitored"),
                         levels = c("Not Monitored", "Monitored"), ordered = TRUE))
colnames(lt) <- c("Provinces", "Logic Test Failures", "Sample Size", "Province Monitored", "Percent Failed at Logic Test")
lt$Provinces[lt$Provinces=="Kandhar"] <- "Kandahar"
lt$Provinces[lt$Provinces=="Sar-i-Pu"] <- "Sar-i-Pul"
lt$Provinces[lt$Provinces=="Uruzghan"] <- "Uruzgan"
tpm <- rename(tpm, Provinces = prov., `Direct observation rating` = `Direct observation`)
tpm$Provinces[tpm$Provinces=="Lagman"] <- "Laghman"

df <- inner_join(tpm, lt, by = "Provinces"); rm(tpm, lt)
```

The logic test results are aggregated at the provincial level for this analysis. In addition to the failtures, we look at the provincial sample size and whether the province was monitored or not (by Sayara). The following table shows part of the data that is used in this analysis.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
kable(head(df), format = "html", align = c("l", "c", "c", "c", "c", "l", "c")) %>% 
  kable_styling(font_size = 13)
```

_**Monitoring Improves Data Quality.**_ Looking at the logic test failure rates by province, and disaggregated by whether the provinces are monitored or not, it becomes evident that the monitored provinces have lower logic test failure rates on average. This means, the quality of data collected at the provinces which were monitored were better comparatively. Note that the monitored provinces were chosen for monitoring because of being more problematic last year. Therefore, it is possible that the imrpovement in data quality as inferred from lower logic test failures are because of the monitoring of the fieldwork. In the following graph, logic test failure rates are compared between provinces that were monitored and provinces that were not monitored in 2017. The plot also include the average rates for the group in the form of horizontal lines.

```{r message=FALSE, warning=FALSE, echo=FALSE}
# calculate the average failure of provinces by monitoring indicator
failure <- group_by(df, `Province Monitored`) %>% summarise(mean_failure = mean(`Percent Failed at Logic Test`))

# plot failed percent for each province using column chart. Color columns by monitoring indicator
ggplot(df, aes(x = Provinces, y = `Percent Failed at Logic Test`, fill = `Province Monitored`)) + geom_col() + 
  
  # separate columns by monitoring indicator. Apply linedraw theme
  facet_wrap(~`Province Monitored`, scales = "free_x") + theme_linedraw() + 
  
  # modify theme: direction of x-axis labels and remove x-axis title, and remove legend
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1), legend.position = "none") + labs(x = "") + 
  
  # add horizontal lines that is average failure of provinces by monitoring indicator
  geom_hline(data = failure, aes(yintercept = mean_failure), col = "navy")
```
