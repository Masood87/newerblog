---
title: Selection of Provinces for Monitoring in SAP 2018
author: Masood Sadat
date: '2018-06-10'
slug: selection-of-provinces-for-monitoring-in-sap-2018
categories:
  - afghanistan
tags:
  - AfghanSurvey
  - EDA
---

```{r include=FALSE}
library(tidyverse)
```

# Introduction

The Asia Foundation (TAF) employs a number of methods to ensure data quality for the Survey of the Afghan People (SAP). The most important quality controls are (1) third party monitoringy, and (2) logic tests. The third party monitoring is carried out on 10% of all surveys by an independent company, and 25% are subject to monitoring from the data collection company. The **third party monitoring** started in 2014, and since then Sayara Research has been the implementing partner for this activity. The **logic tests** are a set of internal tools in the form of codes that are used to detects inconsistencies and potentially fraudulent data. In this note, I document the analysis and reasoning behind the selection of 13 provinces for the _third party monitoring_, in which logic tests and previous year's third party monitoring report are used.

# Data

TAF aims to use monitoring to improve the quality of fieldwork and data, and employs monitoring in the most problematic provinces. In 2017, 13 provinces were monitored, and TAF wants to keep the same number of provinces for 2018 as well. To select 13 most problematic provinces, a number of documents and resources are consulted to finalize the list of provinces. The following are a list of documents and resources consulted for this task:

1. Logic test results in 2017
2. SAP sample in 2017
3. Third Party Monitoring report in 2017
4. Advices from the fieldwork and monitoring companies (ACSOR and Sayara)

# Analysis

```{r echo=FALSE, message=FALSE, warning=FALSE}
lt <- read_csv("~/Documents/SAP 2018/Contracts/Sayara/Province selection based on SAP 2017/SAP 2017 LT report by province.csv", skip = 2)
tpm <- read_csv("~/Documents/SAP 2018/Contracts/Sayara/Province selection based on SAP 2017/sayara scores.csv")

lt <- select(lt, prov., failures, n, sayara, `proportion of LT failures`) %>%
  mutate(`proportion of LT failures` = round(as.numeric(`proportion of LT failures`)*100, 2),
         sayara = factor(case_when(sayara == "0" ~ "Not Monitored", sayara == "1" ~ "Monitored"),
                         levels = c("Not Monitored", "Monitored"), ordered = TRUE))
colnames(lt) <- c("Provinces", "Logic Test Failures", "Sample Size", "Province Monitored", "Percent Failed at Logic Test")
lt$Provinces[lt$Provinces=="Kandhar"] <- "Kandahar"
lt$Provinces[lt$Provinces=="Sar-i-Pu"] <- "Sar-i-Pul"
lt$Provinces[lt$Provinces=="Uruzghan"] <- "Uruzgan"
tpm <- rename(tpm, Provinces = prov., `Direct observation rating` = `Direct observation`)
tpm$Provinces[tpm$Provinces=="Lagman"] <- "Laghman"

df <- inner_join(tpm, lt, by = "Provinces"); rm(tpm, lt)
```

The logic test results are aggregated at the provincial level for this analysis. In addition to the failtures, we look at the provincial sample size and whether the province was monitored or not (by Sayara). The following table shows part of the data that is used in this analysis.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
kable(head(df), format = "html", align = c("l", "c", "c", "c", "c", "l", "c")) %>% 
  kable_styling(font_size = 13)
```

_**Monitoring Improves Data Quality.**_ Looking at the logic test failure rates by province, and disaggregated by whether the provinces are monitored or not, it becomes evident that the monitored provinces have lower logic test failure rates on average. This means, the quality of data collected at the provinces which were monitored were better comparatively. Note that the monitored provinces were chosen for monitoring because of being more problematic last year. Therefore, it is possible that the imrpovement in data quality as inferred from lower logic test failures are because of the monitoring of the fieldwork. In the following graph, logic test failure rates are compared between provinces that were monitored and provinces that were not monitored in 2017. The plot also include the average rates for the group in the form of horizontal lines.

```{r message=FALSE, warning=FALSE, echo=FALSE}
# calculate the average failure of provinces by monitoring indicator
failure <- group_by(df, `Province Monitored`) %>% summarise(mean_failure = mean(`Percent Failed at Logic Test`))

# plot failed percent for each province using column chart. Color columns by monitoring indicator
ggplot(df, aes(x = Provinces, y = `Percent Failed at Logic Test`, fill = `Province Monitored`)) + geom_col() + 
  
  # separate columns by monitoring indicator. Apply linedraw theme
  facet_wrap(~`Province Monitored`, scales = "free_x") + theme_linedraw() + 
  
  # modify theme: direction of x-axis labels and remove x-axis title, and remove legend
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1), legend.position = "none") + labs(x = "") + 
  
  # add horizontal lines that is average failure of provinces by monitoring indicator
  geom_hline(data = failure, aes(yintercept = mean_failure), col = "navy")
```

_**No Relation b/w Monitoring Scores and Logic Tests**_ While having monitors in a province appeared to have a robust effect on the logic test outcomes, monitoring scores of the fieldwork training and fieldwork data collection does not appear to have a robust effect on the logic test outcomes. This is a counterintuitive finding. It was expected that better training and better fieldwork scores would translate into better quality data, which is not the case. There could be three plausable explanation for this finding. 

1. The logic tests do not fully capture the data quality. 
2. The third party monitor's assessment does not accurately capture the quality of the fieldwork. This could mean poor assessment, or assessments done by different monitors in different provinces are not comparable (e.g. different monitors would judge the same quality differently).
3. The sample of monitored provinces is only 13, which is considered small. With small sample size, the amout of information we get is limited and this increases the uncertainty. Therefore, it is possible that we cannot observe meaningful relationship between monitoring scores and the logic test outcomes. Figure below illustrates the correlation between the indicators of interest. The figure shows that the _percent failed at logic test_ have an almost nonexistent relationship with the other two monitoring indicators.

```{r echo=FALSE, message=FALSE, warning=FALSE}
monitoredSelect <- df %>% arrange(desc(`Percent Failed at Logic Test`)) %>%
  top_n(13) %>% filter(`Province Monitored` == "Monitored") %>% nrow()

selectedProv <- df %>% arrange(desc(`Percent Failed at Logic Test`)) %>%
  top_n(13) %>% .[[1]] %>% sort

library(corrplot)
corrplot(corr = cor(df[, c(2, 3, 7)], use = "pairwise"), 
         type = "upper")
```

# Selection of Provinces for Monitoring

Based on last year's results, the best indicator for the data quality (represented by logic test failure rate) is whether third party monitoring was deployed or not. Moreover, the monitoring assessment did not have a meaningful relationship. Listing the most problematic districts (with largest logic test failure rates), irrespective of monitoring, the 13 provinces with largest rates are therefore selected for this year's monitoring. The selected provinces include `r monitoredSelect` provinces that were monitored last year too. The following are the list of 13 provinces selected, and the following plot presents the provinces with their logic test failure rates.

> _Selected provinces for monitoring: `r selectedProv`._

_**There are limitations.**_ It is important to acknowledge that the available data does not provide a complete picture and predict the quality of the fieldwork. Nonetheless, last year's experience provides the best available insight into this year's work to make an informed decision regarding the selection of provinces for monitoring purposes. 

```{r message=FALSE, warning=FALSE, echo=FALSE}
# keep provinces with logic test failures
filter(df, `Percent Failed at Logic Test` > 0) %>% 
  
  # reorder provinces based on their failure rates
  mutate(Provinces = fct_reorder(factor(Provinces), `Percent Failed at Logic Test`)) %>% 
  
  # plot horizontal
  ggplot(aes(Provinces, `Percent Failed at Logic Test`, fill = `Province Monitored`)) + geom_col() + coord_flip() + labs(fill = "") + theme_linedraw() + theme(legend.position = "top") + geom_vline(xintercept = 10.5, col = "navy")
```



















